{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b51adbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# importing beautifulsoup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# importing re\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28a18ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://web.archive.org/web/\"\n",
    "\n",
    "# function to go ahead and construct the urls we'll need here\n",
    "def ret_url(x):\n",
    "    specific_url = base_url + str(x[\"timestamp\"]) + \"/\" + x[\"original\"]\n",
    "    \n",
    "    return(specific_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9bf6ce",
   "metadata": {},
   "source": [
    "### Retrieving the JSON object of URLS to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "810aac73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 542 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# starting url\n",
    "start_url = \"https://web.archive.org/cdx/search/cdx?url=http://www.chinese-embassy.org.uk/eng/PressandMedia/&matchType=prefix&output=json&limit=1000&showResumeKey=true&filter=statuscode:200&collapse=digest\" \n",
    "\n",
    "# using requests to pull the json webpage\n",
    "r = requests.get(start_url)\n",
    "\n",
    "# setting the scrape response to the json from the request\n",
    "scrape_response = r.json()\n",
    "\n",
    "# as long as the last element is a resume key\n",
    "while len(scrape_response[-1]) == 1:\n",
    "    # popping the last item (resume key) & second to last item (blank list)\n",
    "    resume_key = scrape_response.pop(-1)[0]\n",
    "    if len(scrape_response[-1]) == 0:\n",
    "        scrape_response.pop(-1)\n",
    "        \n",
    "    # updating the wayback url\n",
    "    wayback_url = start_url + \"&ResumeKey=\" + resume_key   \n",
    "    \n",
    "    # pulling the resumed query\n",
    "    r = requests.get(wayback_url).json()\n",
    "    \n",
    "    # extending the list of responses\n",
    "    scrape_response.extend(r)\n",
    "    \n",
    "    # sleeping 5 seconds each time to be nice to the server\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b022a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the cdx response to a df\n",
    "scrape_df = pd.DataFrame(scrape_response[1:], columns=scrape_response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6352de74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the duplicates\n",
    "scrape_df.drop_duplicates(subset=\"original\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9617a338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning the storage url\n",
    "scrape_df[\"storage_url\"] = scrape_df.apply(ret_url, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77da7771",
   "metadata": {},
   "source": [
    "### Pulling the elements from a single webpage (to start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9804462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting w/the URL we'll practice this on\n",
    "# we need to pull the col and just the value from the col for now\n",
    "url = scrape_df.head(1)[\"storage_url\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "574f1297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://web.archive.org/web/20211130031323/http://www.chinese-embassy.org.uk/eng/PressandMedia/202008/t20200824_3278215.htm'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f2b554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4ca6447e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.14 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Embassy Spokesperson's Remarks on Chinese operating overseas fishing vessels\",\n",
       " '2020-08-24 23:45',\n",
       " \"Question: According to British media's report, Ecuador on alert over huge Chinese fishing fleet off Galapagos Islands. What is the comment of the Chinese Embassy in the UK?Embassy Spokesperson: China and Ecuador are in friendly communication through bilateral channels. On August 6, the fishery authorities of the two countries held a productive video teleconference, and reached positive consensus. Meanwhile, as a contribution to the protection of fishery resources in the region, China's fishery authority has decided to ban fishing in the high seas west of the Galapagos Islands Marine Reserve from September to November this year, which has been appreciated by Ecuador and other relevant countries.China, as a responsible and big fishing country, attaches great importance to the protection of the marine environment and resources and implements the strictest possible monitoring and control measures on Chinese operating overseas fishing vessels. The competent authority will continue to require enterprises engaged in deep-sea fishing to strictly abide by relevant laws and regulations.\",\n",
       " 'Ambassador Zheng Zeguang']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca170fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1aa57e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# child function\n",
    "# retrieves the title of the page\n",
    "def title_pull(soup):\n",
    "    # pulling all the elements with the bigtitle class that are also td tags (set consists of single element)\n",
    "    title = soup.find_all(\"td\", class_=\"bigtitle\")[0].text\n",
    "    \n",
    "    return(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bcb9dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# child function\n",
    "# retrieves date of release\n",
    "def date_pull(soup):\n",
    "    # pulling all elements that are td tags with set attributes (set consists of single element)\n",
    "    date = soup.find_all(\"td\", height=\"28\", align=\"center\")[0].text\n",
    "    \n",
    "    return(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "94e5cc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# child function\n",
    "# retrieves body of text\n",
    "def body_pull(soup):\n",
    "    # pulling all elements that are p tags (set consists solely of the body of the press release)\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    \n",
    "    # concatenates the paragraphs present in the page. Assumes individual paragraphs are unimportant (may be untrue)\n",
    "    body = \"\".join([x.text for x in paragraphs])\n",
    "    \n",
    "    return(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "20be6b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# child function\n",
    "# retrieves ambassador name\n",
    "def amb_pull(soup):\n",
    "    # pulling the ambassador bio link (correcting to ensure full url)\n",
    "    bio_url = base_url + soup.find_all(href=re.compile(\"/ambassador/dsjl\"))[0][\"href\"][5:]\n",
    "    \n",
    "    # pulling the biography page and setting up a soup object\n",
    "    bio_page = requests.get(bio_url)\n",
    "    bio_soup = BeautifulSoup(bio_page.content, \"html.parser\")\n",
    "    \n",
    "    # pulling out the ambassadors name line from the bio\n",
    "    bios = bio_soup.find_all(class_=\"bigtitle\")\n",
    "    \n",
    "    # running a regex on the title (which will be followed by their name)\n",
    "    ambassador = re.search(r\"(Ambassador.+)\", bios[0].text).group()\n",
    "    \n",
    "    return(ambassador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b509bd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parent function\n",
    "# calls scraping sub-functions\n",
    "# returns dict w/vals for each\n",
    "def scraper(url):\n",
    "    # starting time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # pulling the page of the url and creating a soup object from it\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    # pulling the title, date, body, and ambassador\n",
    "    title = title_pull(soup)\n",
    "    date = date_pull(soup)\n",
    "    body = body_pull(soup)\n",
    "    ambassador = amb_pull(soup)\n",
    "\n",
    "    # calculating the delay\n",
    "    delay = time.time() - start_time\n",
    "    \n",
    "    # pausing for the total time * 2 to be nice to the server we're querying from\n",
    "    time.sleep(delay * 2)\n",
    "    \n",
    "    # returning the pulled stuff from the page(s)\n",
    "    return([title, date, body, ambassador])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6996bf20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
