{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62f61e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "import fasttext\n",
    "import string\n",
    "from pycountry import languages\n",
    "import re\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c223b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ashbu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7afd088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# bringing in the model path for fasttext\n",
    "PRETRAINED_MODEL_PATH = 'lid.176.bin'\n",
    "model = fasttext.load_model(PRETRAINED_MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3952f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to help figure out the language\n",
    "def lang_class(x):\n",
    "    x = x .replace(\"\\n\", \" \")\n",
    "    lang = model.predict(x)\n",
    "    \n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0551988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to help retrieve the language name using the language code\n",
    "def lang_from_code(x):\n",
    "    x = x[0]\n",
    "    x = str(x)\n",
    "  \n",
    "    match = re.match(r\"__label__([a-zA-Z]+)\", x)\n",
    "    match = match.group(1)\n",
    "\n",
    "    if len(match) == 2:\n",
    "        \n",
    "        try:\n",
    "            lang = languages.get(alpha_2=match).name\n",
    "\n",
    "        except:\n",
    "            lang = match\n",
    "    elif len(match) == 3:\n",
    "        try:\n",
    "            lang = languages.get(alpha_3=match).name\n",
    "        \n",
    "        except:   \n",
    "            lang = match\n",
    "        \n",
    "    return(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b34e1356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global vars -> but not too terrible because otherwise it slows down executation by orders of magnitude\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "import string\n",
    "punct = string.punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# pre-proc function to lowercase, rm punct, #s, stopwords, split & lemmatize\n",
    "def pre_proc(x):\n",
    "    # lowercasing the text -> slight possibility of an issue with names...but should be okayish\n",
    "    x = x.lower()\n",
    "   \n",
    "    # removing punctuation\n",
    "    x = x.translate(str.maketrans('', '', punct))\n",
    "    \n",
    "    # removing double-spaces by changing them to single spaces\n",
    "    x = x.replace(\"  \", \" \")\n",
    "    \n",
    "    # removing numbers \n",
    "    x = ''.join(filter(lambda string: not string.isdigit(), x))\n",
    "\n",
    "    # removing the stopwords\n",
    "    x = remove_stopwords(x)\n",
    "        \n",
    "    # splitting the press releases\n",
    "    x = x.split()\n",
    "    \n",
    "    # lemmatizing the words in the releases ->\n",
    "    x = [wordnet_lemmatizer.lemmatize(word) for word in x]\n",
    "    \n",
    "    # joining together the elements in the tokenized list\n",
    "    x = \" \".join(x)  \n",
    "        \n",
    "    # making sure I don't forget to return my result ;) \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "106b5ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the data\n",
    "data = pd.read_json(\"PRC-UK_Embassy_press_releases.ndjson\",lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70408dba",
   "metadata": {},
   "source": [
    "### Looking at the more complicated stuffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f73b05a",
   "metadata": {},
   "source": [
    "#### Fasttext language classification on body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef0915a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 131 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# trying to detecting the languages present here\n",
    "data['bo_lang'] = data['body'].apply(lang_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63f0a6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# splitting the tuples found in the lang_class column\n",
    "data[[\"bo_lang\",\"conf\"]] = pd.DataFrame(data['bo_lang'].tolist(), index=data.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c193d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 33 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# applying func to pull lang code\n",
    "data[\"bo_lang\"] = data[\"bo_lang\"].apply(lang_from_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12a019a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "English    373\n",
       "Name: bo_lang, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"bo_lang\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7add08ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [0.9390600919723511]\n",
       "1      [0.9533370733261108]\n",
       "2       [0.952850878238678]\n",
       "3      [0.9550470113754272]\n",
       "4      [0.9619375467300415]\n",
       "               ...         \n",
       "368    [0.9459292888641357]\n",
       "369    [0.9434953331947327]\n",
       "370    [0.9434953331947327]\n",
       "371    [0.9681368470191956]\n",
       "372    [0.9681368470191956]\n",
       "Name: conf, Length: 373, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to convert the confidence from single deep lists into actual values\n",
    "data[\"conf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f567c049",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "224fb9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# cleaning function -> returns un tokenized text but cleaned w/stopwords removed\n",
    "data[\"cleaned\"] = data[\"body\"].apply(pre_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f060c71",
   "metadata": {},
   "source": [
    "### Single Term Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9d1207",
   "metadata": {},
   "source": [
    "#### Term Frequency overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17123a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vectorizing the posts\n",
    "# initializing the vectorizer\n",
    "# setting min doc freq to 1 release b/c of the small size of the corpus (not setting max df b/c of a fear of losing things)\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "# applying the vectorizer\n",
    "tf = vectorizer.fit_transform(data[\"cleaned\"])\n",
    "\n",
    "# pulling the terms\n",
    "tf_feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# pulling the term counts\n",
    "term_counts = np.asarray(tf.sum(axis=0))[0]\n",
    "\n",
    "# making a dictionary w/terms and term counts\n",
    "term_freqs = dict(zip(tf_feature_names, term_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebd509f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'china': 2349,\n",
       " 'chinese': 1080,\n",
       " 'country': 1062,\n",
       " 'hong': 995,\n",
       " 'kong': 966,\n",
       " 'people': 910,\n",
       " 'xinjiang': 758,\n",
       " 'uk': 753,\n",
       " 'law': 723,\n",
       " 'international': 629,\n",
       " 'right': 591,\n",
       " 'development': 497,\n",
       " 'affair': 489,\n",
       " 'security': 436,\n",
       " 'government': 432,\n",
       " 'national': 414,\n",
       " 'global': 400,\n",
       " 'world': 384,\n",
       " 'ethnic': 372,\n",
       " 'cooperation': 356,\n",
       " 'embassy': 343,\n",
       " 'report': 319,\n",
       " 'time': 310,\n",
       " 'year': 282,\n",
       " 'foreign': 281,\n",
       " 'group': 268,\n",
       " 'state': 268,\n",
       " 'question': 248,\n",
       " 'measure': 240,\n",
       " 'policy': 237,\n",
       " 'internal': 234,\n",
       " 'public': 233,\n",
       " 'region': 233,\n",
       " 'joint': 229,\n",
       " 'covid': 226,\n",
       " 'relevant': 224,\n",
       " 'spokesperson': 223,\n",
       " 'fact': 215,\n",
       " 'economic': 214,\n",
       " 'freedom': 211,\n",
       " 'including': 210,\n",
       " 'issue': 205,\n",
       " 'effort': 204,\n",
       " 'comment': 201,\n",
       " 'community': 198,\n",
       " 'th': 195,\n",
       " 'democracy': 193,\n",
       " 'letter': 191,\n",
       " 'human': 189,\n",
       " 'million': 188}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looks at the top 50 terms to be found here overall\n",
    "# takes advantage of the return type of sorted (a list of tuples) and takes a slice before making a dict from that\n",
    "dict((sorted(term_freqs.items(), key=lambda item: item[1],reverse=True))[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7445a7a7",
   "metadata": {},
   "source": [
    "Looking at the terms and their frequency. We don't see anything too surprising here to a student of global affairs. Quite a bit about China and Chinese, Hong Kong, the UK. Standard terms such as international, right, law, global, government, and security. Question isn't too surprising as it is a common enough term that pops up in the corpus (many of the releases follow a Q&A format to a degree). While \"th\" clearly refers to dates (after I removed the #'s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a4b14a",
   "metadata": {},
   "source": [
    "#### TF by Ambassador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72bbcc46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ambassador Zheng Zeguang', 'Ambassador Liu Xiaoming'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"ambassador\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9a085a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>storage_url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>body</th>\n",
       "      <th>ambassador</th>\n",
       "      <th>bo_lang</th>\n",
       "      <th>conf</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://web.archive.org/web/20211130031323/htt...</td>\n",
       "      <td>Embassy Spokesperson's Remarks on Chinese oper...</td>\n",
       "      <td>2020-08-24 23:45:00</td>\n",
       "      <td>Question: According to British media's report,...</td>\n",
       "      <td>Ambassador Zheng Zeguang</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.9390600919723511]</td>\n",
       "      <td>question according british medias report ecuad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://web.archive.org/web/20211130052058/htt...</td>\n",
       "      <td>Embassy Spokesperson's Remarks on issues relat...</td>\n",
       "      <td>2020-08-25 21:03:00</td>\n",
       "      <td>Question: In an open letter, MPs will be urged...</td>\n",
       "      <td>Ambassador Zheng Zeguang</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.9533370733261108]</td>\n",
       "      <td>question open letter mp urged campaign uk push...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://web.archive.org/web/20211130025452/htt...</td>\n",
       "      <td>Embassy Spokesperson's Remarks on issues relat...</td>\n",
       "      <td>2020-08-25 21:05:00</td>\n",
       "      <td>Question: A letter signed by religious leaders...</td>\n",
       "      <td>Ambassador Zheng Zeguang</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.952850878238678]</td>\n",
       "      <td>question letter signed religious leader differ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://web.archive.org/web/20211130042447/htt...</td>\n",
       "      <td>Embassy Spokesperson's Remarks on The Times' R...</td>\n",
       "      <td>2020-08-29 07:08:00</td>\n",
       "      <td>Question: On 28 August, The Times carried a re...</td>\n",
       "      <td>Ambassador Zheng Zeguang</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.9550470113754272]</td>\n",
       "      <td>question august time carried report saying num...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://web.archive.org/web/20211130054122/htt...</td>\n",
       "      <td>Embassy Spokesperson's Remarks on the Claim by...</td>\n",
       "      <td>2020-09-01 23:40:00</td>\n",
       "      <td>Question: On August 30th, Tom Tugendhat, Chair...</td>\n",
       "      <td>Ambassador Zheng Zeguang</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.9619375467300415]</td>\n",
       "      <td>question august th tom tugendhat chairman fore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>https://web.archive.org/web/20211009215307/htt...</td>\n",
       "      <td>Embassy Spokesperson's Remarks on Some Media R...</td>\n",
       "      <td>2021-10-08 00:00:00</td>\n",
       "      <td>Question: Some media report cites a study that...</td>\n",
       "      <td>Ambassador Zheng Zeguang</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.9542698264122009]</td>\n",
       "      <td>question medium report cite study alleges chin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>https://web.archive.org/web/20211013205341/htt...</td>\n",
       "      <td>Chinese Foreign Ministry Spokesperson's Remark...</td>\n",
       "      <td>2021-10-12 00:00:00</td>\n",
       "      <td>Question: The \"witness cost\" of the so-called ...</td>\n",
       "      <td>Ambassador Zheng Zeguang</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.9676141738891602]</td>\n",
       "      <td>question witness cost socalled uyghur tribunal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>https://web.archive.org/web/20211014135329/htt...</td>\n",
       "      <td>Embassy Spokesperson's Comment on the China-re...</td>\n",
       "      <td>2021-10-14 00:00:00</td>\n",
       "      <td>Question: A few days ago, UK Secretary of the ...</td>\n",
       "      <td>Ambassador Zheng Zeguang</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.9386454224586487]</td>\n",
       "      <td>question day ago uk secretary state foreign co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>https://web.archive.org/web/20211020221311/htt...</td>\n",
       "      <td>Chinese Embassy Spokesperson's Remarks on Ecol...</td>\n",
       "      <td>2021-10-19 00:00:00</td>\n",
       "      <td>Question: The Qinghai-Tibet Plateau is known a...</td>\n",
       "      <td>Ambassador Zheng Zeguang</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.9391968846321106]</td>\n",
       "      <td>question qinghaitibet plateau known pole world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>https://web.archive.org/web/20211024203659/htt...</td>\n",
       "      <td>Embassy Spokesperson's Remarks on the Hong Kon...</td>\n",
       "      <td>2021-10-22 00:00:00</td>\n",
       "      <td>Question: On October 21st, UK Foreign Secretar...</td>\n",
       "      <td>Ambassador Zheng Zeguang</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.9145643711090088]</td>\n",
       "      <td>question october st uk foreign secretary liz t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           storage_url  \\\n",
       "0    https://web.archive.org/web/20211130031323/htt...   \n",
       "1    https://web.archive.org/web/20211130052058/htt...   \n",
       "2    https://web.archive.org/web/20211130025452/htt...   \n",
       "3    https://web.archive.org/web/20211130042447/htt...   \n",
       "4    https://web.archive.org/web/20211130054122/htt...   \n",
       "..                                                 ...   \n",
       "310  https://web.archive.org/web/20211009215307/htt...   \n",
       "311  https://web.archive.org/web/20211013205341/htt...   \n",
       "312  https://web.archive.org/web/20211014135329/htt...   \n",
       "313  https://web.archive.org/web/20211020221311/htt...   \n",
       "314  https://web.archive.org/web/20211024203659/htt...   \n",
       "\n",
       "                                                 title                date  \\\n",
       "0    Embassy Spokesperson's Remarks on Chinese oper... 2020-08-24 23:45:00   \n",
       "1    Embassy Spokesperson's Remarks on issues relat... 2020-08-25 21:03:00   \n",
       "2    Embassy Spokesperson's Remarks on issues relat... 2020-08-25 21:05:00   \n",
       "3    Embassy Spokesperson's Remarks on The Times' R... 2020-08-29 07:08:00   \n",
       "4    Embassy Spokesperson's Remarks on the Claim by... 2020-09-01 23:40:00   \n",
       "..                                                 ...                 ...   \n",
       "310  Embassy Spokesperson's Remarks on Some Media R... 2021-10-08 00:00:00   \n",
       "311  Chinese Foreign Ministry Spokesperson's Remark... 2021-10-12 00:00:00   \n",
       "312  Embassy Spokesperson's Comment on the China-re... 2021-10-14 00:00:00   \n",
       "313  Chinese Embassy Spokesperson's Remarks on Ecol... 2021-10-19 00:00:00   \n",
       "314  Embassy Spokesperson's Remarks on the Hong Kon... 2021-10-22 00:00:00   \n",
       "\n",
       "                                                  body  \\\n",
       "0    Question: According to British media's report,...   \n",
       "1    Question: In an open letter, MPs will be urged...   \n",
       "2    Question: A letter signed by religious leaders...   \n",
       "3    Question: On 28 August, The Times carried a re...   \n",
       "4    Question: On August 30th, Tom Tugendhat, Chair...   \n",
       "..                                                 ...   \n",
       "310  Question: Some media report cites a study that...   \n",
       "311  Question: The \"witness cost\" of the so-called ...   \n",
       "312  Question: A few days ago, UK Secretary of the ...   \n",
       "313  Question: The Qinghai-Tibet Plateau is known a...   \n",
       "314  Question: On October 21st, UK Foreign Secretar...   \n",
       "\n",
       "                   ambassador  bo_lang                  conf  \\\n",
       "0    Ambassador Zheng Zeguang  English  [0.9390600919723511]   \n",
       "1    Ambassador Zheng Zeguang  English  [0.9533370733261108]   \n",
       "2    Ambassador Zheng Zeguang  English   [0.952850878238678]   \n",
       "3    Ambassador Zheng Zeguang  English  [0.9550470113754272]   \n",
       "4    Ambassador Zheng Zeguang  English  [0.9619375467300415]   \n",
       "..                        ...      ...                   ...   \n",
       "310  Ambassador Zheng Zeguang  English  [0.9542698264122009]   \n",
       "311  Ambassador Zheng Zeguang  English  [0.9676141738891602]   \n",
       "312  Ambassador Zheng Zeguang  English  [0.9386454224586487]   \n",
       "313  Ambassador Zheng Zeguang  English  [0.9391968846321106]   \n",
       "314  Ambassador Zheng Zeguang  English  [0.9145643711090088]   \n",
       "\n",
       "                                               cleaned  \n",
       "0    question according british medias report ecuad...  \n",
       "1    question open letter mp urged campaign uk push...  \n",
       "2    question letter signed religious leader differ...  \n",
       "3    question august time carried report saying num...  \n",
       "4    question august th tom tugendhat chairman fore...  \n",
       "..                                                 ...  \n",
       "310  question medium report cite study alleges chin...  \n",
       "311  question witness cost socalled uyghur tribunal...  \n",
       "312  question day ago uk secretary state foreign co...  \n",
       "313  question qinghaitibet plateau known pole world...  \n",
       "314  question october st uk foreign secretary liz t...  \n",
       "\n",
       "[143 rows x 8 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first step is to run it per ambassador and then run the top terms for each of those :(\n",
    "data[data[\"ambassador\"]==\"Ambassador Zheng Zeguang\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2733b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# applying the vectorizer\n",
    "tf = vectorizer.fit_transform(data[\"cleaned\"])\n",
    "\n",
    "# pulling the terms\n",
    "tf_feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# pulling the term counts\n",
    "term_counts = np.asarray(tf.sum(axis=0))[0]\n",
    "\n",
    "# making a dictionary w/terms and term counts\n",
    "term_freqs = dict(zip(tf_feature_names, term_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fad5df5",
   "metadata": {},
   "source": [
    "### Bi-gram & Tri-gram TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f50b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first step is to resplit into unigram - tri trigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff9fcfe",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af969505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretty quick and easy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97012cc7",
   "metadata": {},
   "source": [
    "#### Textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ff821c",
   "metadata": {},
   "source": [
    "#### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25c2f99",
   "metadata": {},
   "source": [
    "#### Vader & NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a3867",
   "metadata": {},
   "source": [
    "#### Visualize by Ambassador's Term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922bec55",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
